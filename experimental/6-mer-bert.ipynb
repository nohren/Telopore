{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda1304b-f36f-451b-8f92-9467bd354f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "1\n",
      "Number of CPU cores: 176\n",
      "PyTorch version: 2.2.2+cu121\n",
      "CUDA version: 12.1\n",
      "torch.cuda.is_available() = True\n",
      "torch.cuda.current_device() = 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "# from torch.amp import autocast, GradScaler\n",
    "!pip install transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW\n",
    "\n",
    "MODEL_NAME = \"zhihan1996/DNA_bert_6\"  # Or another DNABERT variant\n",
    "KMER = 6  # The '6' in DNA_bert_6\n",
    "\n",
    "import os\n",
    "\n",
    "# Must be done *before* torch is imported or used\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "print(os.environ[\"CUDA_LAUNCH_BLOCKING\"])\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print(\"Number of CPU cores:\", num_cores)\n",
    "\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"torch.cuda.is_available() =\", torch.cuda.is_available())\n",
    "print(\"torch.cuda.current_device() =\", torch.cuda.current_device())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b428821-e187-4c35-baf5-46240dcefa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chromosome labels: [ 4 19  0  2  5 22  7  6 15 14  9 21 13  8  3 12 18 11 23 20  1 17 10 16]\n",
      "Max label: 23 Min label: 0\n",
      "tel unique: [1 2 0]\n",
      "0\n",
      "24\n",
      "Empty sequences: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"CHM13_2995.csv\")  # e.g. Code,Chromosome,Telomere,Sequence\n",
    "#print(df.head())\n",
    "\n",
    "# 4.1: Map Chromosome strings to integer IDs\n",
    "df = df.dropna(subset=[\"Chromosome\"])  # drop rows with Chromosome=NaN\n",
    "unique_chrs = df[\"Chromosome\"].unique().tolist()\n",
    "\n",
    "def chr_sort_key(c):\n",
    "    # interpret c as an int if possible, put X=23, Y=24, else bigger\n",
    "    # adjust as you like:\n",
    "    if str(c).isdigit():\n",
    "        return int(c)\n",
    "    elif c == \"X\":\n",
    "        return 23\n",
    "    elif c == \"Y\":\n",
    "        return 24\n",
    "    else:\n",
    "        return 99999\n",
    "\n",
    "sorted_chrs = sorted(unique_chrs, key=chr_sort_key)\n",
    "chr2id = {ch: i for i, ch in enumerate(sorted_chrs)}\n",
    "# After creating chr2id = { \"1\":0, \"2\":1, ..., \"23\":22 } (or with X, Y, etc.)\n",
    "inv_chr2id = {v: k for k, v in chr2id.items()}\n",
    "\n",
    "#print(chr2id)\n",
    "#print(\"num_chr_labels =\", num_chr_labels)    # Should be 24 if your labels are 0..23\n",
    "\n",
    "\n",
    "\n",
    "df[\"chr_label\"] = df[\"Chromosome\"].map(chr2id)\n",
    "\n",
    "# 4.2: Telomere labels are presumably 0/1/2 already\n",
    "df[\"tel_label\"] = df[\"Telomere\"]\n",
    "\n",
    "#print(df)\n",
    "unique_labels = df[\"chr_label\"].unique()\n",
    "print(\"Unique chromosome labels:\", unique_labels)\n",
    "print(\"Max label:\", unique_labels.max(), \"Min label:\", unique_labels.min())\n",
    "print(\"tel unique:\", df[\"tel_label\"].unique())\n",
    "print(df[\"Chromosome\"].isnull().sum())  # or df['chr_label'].isnull().sum()\n",
    "print(len(chr2id))\n",
    "print(\"Empty sequences:\", (df[\"Sequence\"].isnull() | (df[\"Sequence\"] == \"\")).sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "266ea6f7-453e-499b-a1e5-43eac92ac277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chromosome labels: [ 3  8  7  4 12 23 21 19  6 18  0 10  9  1 11  2 16 20  5 17 22 13 15 14]\n",
      "Max label: 23 Min label: 0\n",
      "tel unique: [1 2 0]\n",
      "0\n",
      "24\n",
      "Empty sequences: 0\n"
     ]
    }
   ],
   "source": [
    "df_v = pd.read_csv(\"CN1_2995.csv\")  # e.g. Code,Chromosome,Telomere,Sequence\n",
    "#print(df.head())\n",
    "\n",
    "# 4.1: Map Chromosome strings to integer IDs\n",
    "df_v = df_v.dropna(subset=[\"Chromosome\"])  # drop rows with Chromosome=NaN\n",
    "unique_chrs = df_v[\"Chromosome\"].unique().tolist()\n",
    "\n",
    "def chr_sort_key(c):\n",
    "    # interpret c as an int if possible, put X=23, Y=24, else bigger\n",
    "    # adjust as you like:\n",
    "    if str(c).isdigit():\n",
    "        return int(c)\n",
    "    elif c == \"X\":\n",
    "        return 23\n",
    "    elif c == \"Y\":\n",
    "        return 24\n",
    "    else:\n",
    "        return 99999\n",
    "\n",
    "sorted_chrs = sorted(unique_chrs, key=chr_sort_key)\n",
    "chr2id = {ch: i for i, ch in enumerate(sorted_chrs)}\n",
    "# After creating chr2id = { \"1\":0, \"2\":1, ..., \"23\":22 } (or with X, Y, etc.)\n",
    "inv_chr2id = {v: k for k, v in chr2id.items()}\n",
    "\n",
    "#print(chr2id)\n",
    "#print(\"num_chr_labels =\", num_chr_labels)    # Should be 24 if your labels are 0..23\n",
    "\n",
    "\n",
    "\n",
    "df_v[\"chr_label\"] = df_v[\"Chromosome\"].map(chr2id)\n",
    "\n",
    "# 4.2: Telomere labels are presumably 0/1/2 already\n",
    "df_v[\"tel_label\"] = df_v[\"Telomere\"]\n",
    "\n",
    "#print(df_v)\n",
    "unique_labels = df_v[\"chr_label\"].unique()\n",
    "print(\"Unique chromosome labels:\", unique_labels)\n",
    "print(\"Max label:\", unique_labels.max(), \"Min label:\", unique_labels.min())\n",
    "print(\"tel unique:\", df_v[\"tel_label\"].unique())\n",
    "print(df[\"Chromosome\"].isnull().sum())  # or df['chr_label'].isnull().sum()\n",
    "print(len(chr2id))\n",
    "print(\"Empty sequences:\", (df_v[\"Sequence\"].isnull() | (df_v[\"Sequence\"] == \"\")).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8ea53e-a0d3-4901-a339-bc6c24401335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The creators of DNABERT released separate checkpoints for k=3, k=4, k=5, k=6, (and sometimes k=7)\n",
    "#  because different k‚Äêmer sizes can capture different types of patterns in the DNA. \n",
    "\n",
    "def seq_to_kmers(seq, k=6):\n",
    "    \"\"\"\n",
    "    Convert a DNA sequence into overlapping k-mers,\n",
    "    then join them with spaces for DNABERT's tokenizer.\n",
    "    \"\"\"\n",
    "    seq = seq.upper()\n",
    "    kmers = []\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        kmers.append(seq[i:i+k])\n",
    "    return \" \".join(kmers)\n",
    "\n",
    "######################################\n",
    "#  B. Chunking + K-mer Helpers\n",
    "######################################\n",
    "def chunk_sequence(seq, chunk_size=512, overlap=50):\n",
    "    \"\"\"\n",
    "    Return a list of overlapping substrings from `seq`.\n",
    "    Example: first chunk covers [0:512], next chunk covers [462:974], etc.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    length = len(seq)\n",
    "    while start < length:\n",
    "        end = start + chunk_size\n",
    "        chunk = seq[start:end]\n",
    "        chunks.append(chunk)\n",
    "        if end >= length:\n",
    "            break\n",
    "        # move to next chunk with overlap\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1127851c-aa56-481e-b5d8-aae378af855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNABertChunkedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    - Each row in the CSV can produce multiple chunk-examples (if the sequence is long).\n",
    "    - We'll store them as separate items in the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, k=6, chunk_size=512, overlap=50, max_length=512):\n",
    "        self.samples = []  # will hold dicts of form: { \"kmers_str\":..., \"chr_label\":..., \"tel_label\":... }\n",
    "        self.tokenizer = tokenizer\n",
    "        self.k = k\n",
    "        self.max_length = max_length\n",
    "\n",
    "        for idx in range(len(df)):\n",
    "            row = df.iloc[idx]\n",
    "            sequence = row[\"Sequence\"]\n",
    "            chr_label = row[\"chr_label\"]\n",
    "            tel_label = row[\"tel_label\"]\n",
    "\n",
    "            # Split the full sequence into overlapping chunks\n",
    "            seq_chunks = chunk_sequence(sequence, chunk_size=chunk_size, overlap=overlap)\n",
    "            for ch in seq_chunks:\n",
    "                if len(ch) < self.k:\n",
    "                    continue\n",
    "                # Convert that chunk to k-mer text\n",
    "                kmers_str = seq_to_kmers(ch, k=self.k)\n",
    "                self.samples.append({\n",
    "                    \"kmers_str\": kmers_str,\n",
    "                    \"chr_label\": chr_label,\n",
    "                    \"tel_label\": tel_label\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        kmers_str = item[\"kmers_str\"]\n",
    "        chr_label = item[\"chr_label\"]\n",
    "        tel_label = item[\"tel_label\"]\n",
    "\n",
    "        # Tokenize the k-mer string\n",
    "        encoding = self.tokenizer(\n",
    "            kmers_str,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels_chr': torch.tensor(chr_label, dtype=torch.long),\n",
    "            'labels_tel': torch.tensor(tel_label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e66a823d-465e-42f8-be20-1e120a26ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#  D. Multi-Task Model\n",
    "######################################\n",
    "class MultiTaskDNABERT(nn.Module):\n",
    "    def __init__(self, model_name, num_chr_labels, num_tel_labels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.bert = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "\n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Two classification heads\n",
    "        self.classifier_chr = nn.Linear(hidden_size, num_chr_labels)\n",
    "        self.classifier_tel = nn.Linear(hidden_size, num_tel_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels_chr=None, labels_tel=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        # if there's a pooler_output, use it, else fallback to [CLS]\n",
    "        if outputs.pooler_output is not None:\n",
    "            pooled = outputs.pooler_output\n",
    "        else:\n",
    "            pooled = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        x = self.dropout(pooled)\n",
    "        logits_chr = self.classifier_chr(x)\n",
    "        logits_tel = self.classifier_tel(x)\n",
    "\n",
    "        loss = None\n",
    "        if labels_chr is not None and labels_tel is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss_chr = loss_fct(logits_chr, labels_chr)\n",
    "            loss_tel = loss_fct(logits_tel, labels_tel)\n",
    "            loss = loss_chr + loss_tel\n",
    "\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits_chr': logits_chr,\n",
    "            'logits_tel': logits_tel\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a6d16e2-b468-4806-b7b8-60163ca97190",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#  E. Prepare Data\n",
    "######################################\n",
    "MODEL_NAME = \"zhihan1996/DNA_bert_6\"  # Or a local checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "num_chr_labels = len(chr2id)  # e.g. ~24 if \"1..22,X,Y\"\n",
    "num_tel_labels = 3            # 0,1,2\n",
    "\n",
    "train_ds = DNABertChunkedDataset(\n",
    "    df=df,\n",
    "    tokenizer=tokenizer,\n",
    "    k=6,\n",
    "    chunk_size=512,   # chunk char length\n",
    "    overlap=50,       # overlap in chars\n",
    "    max_length=512    # DNABERT max token input\n",
    ")\n",
    "\n",
    "valid_ds = DNABertChunkedDataset(\n",
    "    df=df_v,\n",
    "    tokenizer=tokenizer,\n",
    "    k=6,\n",
    "    chunk_size=512,   # chunk char length\n",
    "    overlap=50,       # overlap in chars\n",
    "    max_length=512    # DNABERT max token input\n",
    ")\n",
    "\n",
    "val_size = int(0.5 * len(valid_ds))\n",
    "leftover = len(valid_ds) - val_size\n",
    "half_val_ds, leftover_ds = random_split(valid_ds, [val_size, leftover])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=num_cores-1)\n",
    "val_loader = DataLoader(half_val_ds, batch_size=16, shuffle=False, num_workers=num_cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331f2b6f-3bc7-4029-b486-9b54c9cb3b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 00:54:53.192564: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-13 00:54:53.220707: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-13 00:54:53.771868: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 out of 2958\n",
      "Batch 200 out of 2958\n",
      "Batch 400 out of 2958\n",
      "Batch 600 out of 2958\n",
      "Batch 800 out of 2958\n",
      "Batch 1000 out of 2958\n",
      "Batch 1200 out of 2958\n",
      "Batch 1400 out of 2958\n",
      "Batch 1600 out of 2958\n",
      "Batch 1800 out of 2958\n",
      "Batch 2000 out of 2958\n",
      "Batch 2200 out of 2958\n",
      "Batch 2400 out of 2958\n",
      "Batch 2600 out of 2958\n",
      "Batch 2800 out of 2958\n",
      "Epoch 1 - Train Loss: 2.9420\n",
      "Val Loss: 3.4307, Chr Acc: 0.181, Tel Acc: 0.795\n",
      "Batch 0 out of 2958\n",
      "Batch 200 out of 2958\n",
      "Batch 400 out of 2958\n",
      "Batch 600 out of 2958\n",
      "Batch 800 out of 2958\n",
      "Batch 1000 out of 2958\n",
      "Batch 1200 out of 2958\n",
      "Batch 1400 out of 2958\n",
      "Batch 1600 out of 2958\n",
      "Batch 1800 out of 2958\n",
      "Batch 2000 out of 2958\n",
      "Batch 2200 out of 2958\n",
      "Batch 2400 out of 2958\n",
      "Batch 2600 out of 2958\n",
      "Batch 2800 out of 2958\n",
      "Epoch 2 - Train Loss: 2.0188\n",
      "Val Loss: 3.3874, Chr Acc: 0.290, Tel Acc: 0.799\n",
      "Batch 0 out of 2958\n",
      "Batch 200 out of 2958\n",
      "Batch 400 out of 2958\n",
      "Batch 600 out of 2958\n",
      "Batch 800 out of 2958\n",
      "Batch 1000 out of 2958\n",
      "Batch 1200 out of 2958\n",
      "Batch 1400 out of 2958\n",
      "Batch 1600 out of 2958\n",
      "Batch 1800 out of 2958\n",
      "Batch 2000 out of 2958\n",
      "Batch 2200 out of 2958\n",
      "Batch 2400 out of 2958\n",
      "Batch 2600 out of 2958\n",
      "Batch 2800 out of 2958\n",
      "Epoch 3 - Train Loss: 1.5391\n",
      "Val Loss: 3.4972, Chr Acc: 0.337, Tel Acc: 0.798\n",
      "Batch 0 out of 2958\n",
      "Batch 200 out of 2958\n",
      "Batch 400 out of 2958\n",
      "Batch 600 out of 2958\n",
      "Batch 800 out of 2958\n",
      "Batch 1000 out of 2958\n",
      "Batch 1200 out of 2958\n",
      "Batch 1400 out of 2958\n",
      "Batch 1600 out of 2958\n",
      "Batch 1800 out of 2958\n",
      "Batch 2000 out of 2958\n",
      "Batch 2200 out of 2958\n",
      "Batch 2400 out of 2958\n",
      "Batch 2600 out of 2958\n",
      "Batch 2800 out of 2958\n",
      "Epoch 4 - Train Loss: 1.2973\n",
      "Val Loss: 3.5427, Chr Acc: 0.357, Tel Acc: 0.800\n",
      "Batch 0 out of 2958\n",
      "Batch 200 out of 2958\n",
      "Batch 400 out of 2958\n",
      "Batch 600 out of 2958\n",
      "Batch 800 out of 2958\n",
      "Batch 1000 out of 2958\n",
      "Batch 1200 out of 2958\n",
      "Batch 1400 out of 2958\n",
      "Batch 1600 out of 2958\n",
      "Batch 1800 out of 2958\n",
      "Batch 2000 out of 2958\n",
      "Batch 2200 out of 2958\n",
      "Batch 2400 out of 2958\n",
      "Batch 2600 out of 2958\n",
      "Batch 2800 out of 2958\n",
      "Epoch 5 - Train Loss: 1.1687\n",
      "Val Loss: 3.6873, Chr Acc: 0.364, Tel Acc: 0.799\n",
      "Batch 0 out of 2958\n",
      "Batch 200 out of 2958\n",
      "Batch 400 out of 2958\n",
      "Batch 600 out of 2958\n",
      "Batch 800 out of 2958\n",
      "Batch 1000 out of 2958\n",
      "Batch 1200 out of 2958\n",
      "Batch 1400 out of 2958\n",
      "Batch 1600 out of 2958\n",
      "Batch 1800 out of 2958\n",
      "Batch 2000 out of 2958\n",
      "Batch 2200 out of 2958\n",
      "Batch 2400 out of 2958\n",
      "Batch 2600 out of 2958\n",
      "Batch 2800 out of 2958\n",
      "Epoch 6 - Train Loss: 1.0938\n",
      "Val Loss: 3.7622, Chr Acc: 0.366, Tel Acc: 0.795\n",
      "Batch 0 out of 2958\n",
      "Batch 200 out of 2958\n",
      "Batch 400 out of 2958\n",
      "Batch 600 out of 2958\n",
      "Batch 800 out of 2958\n",
      "Batch 1000 out of 2958\n",
      "Batch 1200 out of 2958\n",
      "Batch 1400 out of 2958\n",
      "Batch 1600 out of 2958\n",
      "Batch 1800 out of 2958\n",
      "Batch 2000 out of 2958\n",
      "Batch 2200 out of 2958\n",
      "Batch 2400 out of 2958\n",
      "Batch 2600 out of 2958\n",
      "Batch 2800 out of 2958\n",
      "Epoch 7 - Train Loss: 1.0196\n",
      "Val Loss: 3.8049, Chr Acc: 0.385, Tel Acc: 0.798\n",
      "Batch 0 out of 2958\n",
      "Batch 200 out of 2958\n",
      "Batch 400 out of 2958\n",
      "Batch 600 out of 2958\n",
      "Batch 800 out of 2958\n",
      "Batch 1000 out of 2958\n",
      "Batch 1200 out of 2958\n",
      "Batch 1400 out of 2958\n",
      "Batch 1600 out of 2958\n",
      "Batch 1800 out of 2958\n",
      "Batch 2000 out of 2958\n",
      "Batch 2200 out of 2958\n",
      "Batch 2400 out of 2958\n",
      "Batch 2600 out of 2958\n",
      "Batch 2800 out of 2958\n",
      "Epoch 8 - Train Loss: 0.9639\n",
      "Val Loss: 3.9010, Chr Acc: 0.388, Tel Acc: 0.797\n",
      "Batch 0 out of 2958\n",
      "Batch 200 out of 2958\n",
      "Batch 400 out of 2958\n",
      "Batch 600 out of 2958\n",
      "Batch 800 out of 2958\n",
      "Batch 1000 out of 2958\n",
      "Batch 1200 out of 2958\n",
      "Batch 1400 out of 2958\n",
      "Batch 1600 out of 2958\n",
      "Batch 1800 out of 2958\n",
      "Batch 2000 out of 2958\n",
      "Batch 2200 out of 2958\n",
      "Batch 2400 out of 2958\n",
      "Batch 2600 out of 2958\n",
      "Batch 2800 out of 2958\n",
      "Epoch 9 - Train Loss: 0.9164\n",
      "Val Loss: 3.9704, Chr Acc: 0.396, Tel Acc: 0.799\n",
      "Batch 0 out of 2958\n",
      "Batch 200 out of 2958\n",
      "Batch 400 out of 2958\n",
      "Batch 600 out of 2958\n",
      "Batch 800 out of 2958\n",
      "Batch 1000 out of 2958\n",
      "Batch 1200 out of 2958\n",
      "Batch 1400 out of 2958\n",
      "Batch 1600 out of 2958\n",
      "Batch 1800 out of 2958\n",
      "Batch 2000 out of 2958\n",
      "Batch 2200 out of 2958\n",
      "Batch 2400 out of 2958\n",
      "Batch 2600 out of 2958\n",
      "Batch 2800 out of 2958\n",
      "Epoch 10 - Train Loss: 0.8838\n",
      "Val Loss: 3.9848, Chr Acc: 0.395, Tel Acc: 0.798\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "#  F. Initialize Model & Train\n",
    "######################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultiTaskDNABERT(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_chr_labels=num_chr_labels,\n",
    "    num_tel_labels=num_tel_labels\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "#cosine decay with warmup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "batches_per_epoch = len(train_loader)  # e.g. 100\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=batches_per_epoch,  # after T_0 steps, it restarts\n",
    "    T_mult=1,               # no extension of the cycle length each time\n",
    "    eta_min=0               # the minimum LR at the cosine nadir\n",
    ")\n",
    "\n",
    "\n",
    "best_acc_chrom = 0\n",
    "EPOCHS = 10\n",
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            if batch_idx % 200 == 0:\n",
    "                print(f\"Batch {batch_idx} out of {len(train_loader)}\")\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_chr = batch['labels_chr'].to(device)\n",
    "            labels_tel = batch['labels_tel'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels_chr=labels_chr,\n",
    "                labels_tel=labels_tel\n",
    "            )\n",
    "            loss = outputs['loss']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_chr, correct_tel = 0, 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels_chr = batch['labels_chr'].to(device)\n",
    "                labels_tel = batch['labels_tel'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels_chr=labels_chr,\n",
    "                    labels_tel=labels_tel\n",
    "                )\n",
    "                loss = outputs['loss']\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                logits_chr = outputs['logits_chr']\n",
    "                logits_tel = outputs['logits_tel']\n",
    "                preds_chr = torch.argmax(logits_chr, dim=1)\n",
    "                preds_tel = torch.argmax(logits_tel, dim=1)\n",
    "\n",
    "                correct_chr += (preds_chr == labels_chr).sum().item()\n",
    "                correct_tel += (preds_tel == labels_tel).sum().item()\n",
    "                total_samples += len(labels_chr)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        acc_chr = correct_chr / total_samples\n",
    "        acc_tel = correct_tel / total_samples\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Chr Acc: {acc_chr:.3f}, Tel Acc: {acc_tel:.3f}\")\n",
    "        if acc_chr > best_acc_chrom:\n",
    "            best_acc_chrom = acc_chr\n",
    "            torch.save(model.state_dict(), \"model_best.pt\")\n",
    "except KeyboardInterrupt:\n",
    "   # print(\"Training interrupted; saving partial model.\")\n",
    "    #torch.save(model.state_dict(), \"my_partial_model.pt\")\n",
    "    print('interrupted')\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e82d97-9113-4ca1-887a-ee4c0285cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#  G. (Optional) Chunk-Level -> Sequence-Level Aggregation\n",
    "######################################\n",
    "# If each sequence was chunked into N pieces, you might want an overall label for the entire sequence.\n",
    "# One approach: gather chunk predictions for the same \"Code\" or row, then do majority vote or average logits.\n",
    "# This is a quick example of how you might do it for \"val_ds\".\n",
    "\n",
    "def predict_sequence(model, tokenizer, seq, k=6, chunk_size=512, overlap=50):\n",
    "    \"\"\"\n",
    "    Return average logits across all chunks for (chr, tel).\n",
    "    \"\"\"\n",
    "    seq_chunks = chunk_sequence(seq, chunk_size=chunk_size, overlap=overlap)\n",
    "    model.eval()\n",
    "\n",
    "    sum_logits_chr = None\n",
    "    sum_logits_tel = None\n",
    "    total_chunks = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ch in seq_chunks:\n",
    "            if len(ch) < k:\n",
    "                    continue\n",
    "            kmers_str = seq_to_kmers(ch, k=k)\n",
    "            encoding = tokenizer(\n",
    "                kmers_str,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=512\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits_chr = outputs['logits_chr']\n",
    "            logits_tel = outputs['logits_tel']\n",
    "\n",
    "            if sum_logits_chr is None:\n",
    "                sum_logits_chr = logits_chr\n",
    "                sum_logits_tel = logits_tel\n",
    "            else:\n",
    "                sum_logits_chr += logits_chr\n",
    "                sum_logits_tel += logits_tel\n",
    "            total_chunks += 1\n",
    "\n",
    "    avg_chr = sum_logits_chr / total_chunks\n",
    "    avg_tel = sum_logits_tel / total_chunks\n",
    "    pred_chr_id = torch.argmax(avg_chr, dim=1).item()\n",
    "    chr_str = inv_chr2id[pred_chr_id]\n",
    "    pred_tel_id = torch.argmax(avg_tel, dim=1).item()\n",
    "    return chr_str, pred_tel_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d23ac-2c94-431a-928a-c7dc1058f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = df.loc[1, \"Sequence\"]\n",
    "# chrom = df.loc[1, \"Chromosome\"]\n",
    "# telo = df.loc[1, \"Telomere\"]\n",
    "\n",
    "print('ground truth', chrom, telo)\n",
    "\n",
    "#accuracy \n",
    "acc = []\n",
    "for index, row in df_v.iterrows():\n",
    "    # index is the row index (int)\n",
    "    # row is a Series with columns as keys\n",
    "    sequence = row[\"Sequence\"]\n",
    "    chrom = row[\"Chromosome\"]\n",
    "    telo  = row[\"Telomere\"]\n",
    "    # do something with these values\n",
    "    chr_str, tel_label = predict_sequence(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        seq=sequence,  # your test DNA sequence\n",
    "        k=6,                  # must match your training k-mer\n",
    "        chunk_size=512,       # same chunk size as training\n",
    "        overlap=50            # same overlap as training\n",
    "    )\n",
    "    if chr_str == chrom and tel_label == telo:\n",
    "        acc.append(1)\n",
    "    else:\n",
    "        acc.append(0)\n",
    "\n",
    "\n",
    "print('acc',sum(acc)/len(acc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print('pred', chr_str, tel_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
